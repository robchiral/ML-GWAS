{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "\n",
    "import itertools\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Custom functions\n",
    "\n",
    "def id_cat_feat(df):\n",
    "    return [col for col in df.columns if 2 < df[col].nunique() < 10 and \"days\" not in col]\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def permutation_test(y_true, model1_scores, model2_scores, n_permutations=1000):\n",
    "    observed_diff = average_precision_score(y_true, model1_scores) - average_precision_score(y_true, model2_scores)\n",
    "    n_model1 = len(model1_scores)\n",
    "    n_model2 = len(model2_scores)\n",
    "    all_scores = np.concatenate([model1_scores, model2_scores])\n",
    "    shuffled_diffs = np.empty(n_permutations)\n",
    "\n",
    "    for i in range(n_permutations):\n",
    "        np.random.shuffle(all_scores)\n",
    "        shuffled_model1_scores = all_scores[:n_model1]\n",
    "        shuffled_model2_scores = all_scores[n_model1:]\n",
    "        shuffled_diffs[i] = average_precision_score(y_true, shuffled_model1_scores) - average_precision_score(y_true, shuffled_model2_scores)\n",
    "\n",
    "    p_value = np.mean(np.abs(shuffled_diffs) >= np.abs(observed_diff))\n",
    "    return p_value\n",
    "\n",
    "def bootstrap(dataframe, predictor_col, outcome_col, metric_function, n_bootstrap=500, resample_frac=1, confidence_level=0.95):\n",
    "    sample_size = int(len(dataframe) * resample_frac)\n",
    "    metric_values = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = dataframe.sample(n=sample_size, replace=True)\n",
    "        metric_value = metric_function(sample[outcome_col], sample[predictor_col])\n",
    "        metric_values.append(metric_value)\n",
    "\n",
    "    metric_values = np.array(metric_values)\n",
    "    mean = np.mean(metric_values)\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_bound = np.percentile(metric_values, 100 * alpha / 2)\n",
    "    upper_bound = np.percentile(metric_values, 100 * (1 - alpha / 2))\n",
    "\n",
    "    return mean, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dx in ['afib','cad','celiac','gallstone','polyp','t2d','varicose','vte']:\n",
    "    \n",
    "    dx_name = dx+'_status'\n",
    "\n",
    "    cd = pd.read_pickle('./Final/cases.pkl')\n",
    "    indata = pd.read_pickle('./Final/indata.pkl')\n",
    "\n",
    "    indata = indata.merge(cd[['f.eid',dx_name]], on='f.eid', how='inner').drop_duplicates('f.eid')\n",
    "    indata = indata.loc[indata[dx_name] != -1]\n",
    "    indata.loc[indata[dx_name].isna(), dx_name] = 0\n",
    "\n",
    "    if dx == 'afib':\n",
    "        indata = indata.drop(['I48','C01A','C01B','B01A'],axis=1)\n",
    "    if dx == 'cad':\n",
    "        indata = indata.drop(['C01D','I20','I21','I22','I23','I24','I25'],axis=1)\n",
    "    if dx == 'celiac':\n",
    "        indata = indata.drop(['K90'],axis=1)\n",
    "    if dx == 'gallstone':\n",
    "        indata = indata.drop(['K80'],axis=1)\n",
    "    if dx == 'polyp':\n",
    "        indata = indata.drop(['J33'],axis=1)\n",
    "    if dx == 't2d':\n",
    "        indata = indata.drop(['E11','E14','A10B'],axis=1)\n",
    "    if dx == 'varicose':\n",
    "        indata = indata.drop(['I83'],axis=1)\n",
    "    if dx == 'vte':\n",
    "        indata = indata.drop(['I80','I81','I82','I26','B01A'],axis=1)\n",
    "        \n",
    "    indata = indata.dropna(thresh=100,axis=1)\n",
    "\n",
    "    ##### Run ML\n",
    "    \n",
    "    condition = 'All features'\n",
    "\n",
    "    X = indata.drop(['f.eid', dx_name],axis=1).astype(float)\n",
    "    col_names = X.columns.to_list()\n",
    "    X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_.\\ ]+', '', x))\n",
    "    cat_feat = id_cat_feat(X)\n",
    "    y = indata[dx_name].astype(float)\n",
    "    ids = indata['f.eid']\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    #####\n",
    "    \n",
    "    params_test = {\n",
    "        'data_sample_strategy': ['goss'],\n",
    "        'boosting': ['gbdt'],\n",
    "        'objective': ['binary'],\n",
    "        'force_col_wise': ['true'],    \n",
    "        'num_iterations': [1000],\n",
    "        'learning_rate': [0.1],\n",
    "        'early_stopping_round': [5],\n",
    "        'verbose': [0],\n",
    "        'num_leaves': [20, 35, 50, 65, 80],\n",
    "        'min_data_in_leaf': [50, 100, 500],\n",
    "        'lambda_l2': [1, 0.1, 10, 100],\n",
    "        'feature_fraction': [1, 0.9, 0.8]\n",
    "    }\n",
    "    keys, values = zip(*params_test.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    df = pd.DataFrame(combinations)\n",
    "    param_list = df.to_dict(orient='records')\n",
    "\n",
    "    metrics = []\n",
    "    \n",
    "    for params in param_list:\n",
    "    \n",
    "        outer_cv = KFold(n_splits=6, shuffle=True)\n",
    "        for outer_fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(X)):\n",
    "            X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "            y_train_val, y_test = y.iloc[train_val_idx], y.iloc[test_idx]\n",
    "            ids_test = ids.iloc[test_idx]\n",
    "    \n",
    "            inner_cv = KFold(n_splits=6, shuffle=True)\n",
    "            for inner_fold, (train_idx, val_idx) in enumerate(inner_cv.split(X_train_val)):\n",
    "                \n",
    "                X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "                y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "                \n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            \n",
    "                # Model training\n",
    "                model = lgb.train(params, train_data, valid_sets=val_data, categorical_feature=cat_feat)\n",
    "                best_iter = model.best_iteration\n",
    "\n",
    "                # Prediction & Evaluation on validation set\n",
    "                y_pred_val = model.predict(X_val, num_iteration=best_iter)\n",
    "                auroc_val = roc_auc_score(y_val, y_pred_val)\n",
    "                auprc_val = average_precision_score(y_val, y_pred_val)\n",
    "                ll_val = log_loss(y_val, y_pred_val)\n",
    "        \n",
    "                # Prediction & Evaluation on holdout set\n",
    "                y_pred_hold = model.predict(X_test, num_iteration=best_iter)\n",
    "                auroc_hold = roc_auc_score(y_test, y_pred_hold)\n",
    "                auprc_hold = average_precision_score(y_test, y_pred_hold)\n",
    "                ll_hold = log_loss(y_test, y_pred_hold)\n",
    "                        \n",
    "                # Collecting metrics\n",
    "                filtered_params = {k: params[k] for k in ['num_leaves', 'min_data_in_leaf', 'lambda_l2', 'feature_fraction']}\n",
    "                metrics_dict = {'Disease': dx,\n",
    "                                'Outer fold': outer_fold, 'Inner fold': inner_fold,\n",
    "                                'AUROC_val': auroc_val, 'AUPRC_val': auprc_val, 'LL_val': ll_val, 'Prop_val': y_val.mean(),\n",
    "                                'AUROC_hold': auroc_hold, 'AUPRC_hold': auprc_hold, 'LL_hold': ll_hold, 'Prop_hold': y_test.mean()}\n",
    "                metrics_dict = {**filtered_params, **metrics_dict}\n",
    "                metrics.append(metrics_dict)\n",
    "                print(metrics_dict)\n",
    "                \n",
    "    ##### Save data\n",
    "        \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_pickle(f'./ML_Results/HP_Search/{dx_name}_metrics.pkl')\n",
    "    metrics = 0\n",
    "    metrics_df = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dx in ['afib','cad','celiac','gallstone','polyp','t2d','varicose','vte']:\n",
    "\n",
    "    dx_name = dx+'_status'\n",
    "\n",
    "    cd = pd.read_pickle('./Final/cases.pkl')\n",
    "    indata = pd.read_pickle('./Final/indata.pkl')\n",
    "\n",
    "    indata = indata.merge(cd[['f.eid',dx_name]], on='f.eid', how='inner').drop_duplicates('f.eid')\n",
    "    indata = indata.loc[indata[dx_name] != -1]\n",
    "    indata.loc[indata[dx_name].isna(), dx_name] = 0\n",
    "\n",
    "    if dx == 'afib':\n",
    "        indata = indata.drop(['I48','C01A','C01B','B01A'],axis=1)\n",
    "    if dx == 'cad':\n",
    "        indata = indata.drop(['C01D','I20','I21','I22','I23','I24','I25'],axis=1)\n",
    "    if dx == 'celiac':\n",
    "        indata = indata.drop(['K90'],axis=1)\n",
    "    if dx == 'gallstone':\n",
    "        indata = indata.drop(['K80'],axis=1)\n",
    "    if dx == 'polyp':\n",
    "        indata = indata.drop(['J33'],axis=1)\n",
    "    if dx == 't2d':\n",
    "        indata = indata.drop(['E11','E14','A10B'],axis=1)\n",
    "    if dx == 'varicose':\n",
    "        indata = indata.drop(['I83'],axis=1)\n",
    "    if dx == 'vte':\n",
    "        indata = indata.drop(['I80','I81','I82','I26','B01A'],axis=1)\n",
    "        \n",
    "    indata = indata.dropna(thresh=100,axis=1)\n",
    "\n",
    "    ##### Run ML\n",
    "    \n",
    "    condition = 'All features'\n",
    "\n",
    "    X = indata.drop(['f.eid', dx_name],axis=1).astype(float)\n",
    "    col_names = X.columns.to_list()\n",
    "    X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_.\\ ]+', '', x))\n",
    "    cat_feat = id_cat_feat(X)\n",
    "    y = indata[dx_name].astype(float)\n",
    "    ids = indata['f.eid']\n",
    "\n",
    "    # Placeholder for metrics and predictions\n",
    "    metrics = []\n",
    "    predictions = []\n",
    "    importances = []\n",
    "    hold_shap = []\n",
    "    hold_ids = []\n",
    "\n",
    "    # Outer 6-fold CV\n",
    "    outer_cv = KFold(n_splits=6, shuffle=True)\n",
    "    for outer_fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(X)):\n",
    "        X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "        y_train_val, y_test = y.iloc[train_val_idx], y.iloc[test_idx]\n",
    "        ids_test = ids.iloc[test_idx]\n",
    "\n",
    "        # Inner 6-fold CV (90% train/validation, 10% validation)\n",
    "        inner_cv = KFold(n_splits=6, shuffle=True)\n",
    "        for inner_fold, (train_idx, val_idx) in enumerate(inner_cv.split(X_train_val)):\n",
    "            X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "            y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "            \n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            \n",
    "            params = {\n",
    "                'data_sample_strategy': 'goss',\n",
    "                'boosting': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'force_col_wise': 'true',\n",
    "                'num_iterations': 1000,\n",
    "                'num_leaves': 50,\n",
    "                'learning_rate': 0.1,\n",
    "                'min_data_in_leaf': 100,\n",
    "                'early_stopping_round': 5,\n",
    "                'lambda_l2': 1,\n",
    "                'verbose': 0,\n",
    "                'feature_fraction': 1\n",
    "            }\n",
    "        \n",
    "            # Model training\n",
    "            if True:\n",
    "                model = lgb.train(params, train_data, valid_sets=val_data, categorical_feature=cat_feat)\n",
    "                model.save_model(f'./ML_Results/Models/{dx_name}_{condition}_{outer_fold}_{inner_fold}.model', num_iteration=model.best_iteration)\n",
    "            \n",
    "            if False:\n",
    "                model = lgb.Booster(model_file=f'./ML_Results/Models/{dx_name}_{condition}_{outer_fold}_{inner_fold}.model')\n",
    "            \n",
    "            importances.append(dict(zip(col_names, model.feature_importance(importance_type='gain'))))\n",
    "            if inner_fold == 0:\n",
    "                hold_shap.append(model.predict(X_test, num_iteration=model.best_iteration, pred_contrib=True)[:, :-1])\n",
    "                hold_ids.append(ids_test)    \n",
    "            \n",
    "            # Prediction & Evaluation on validation set\n",
    "            y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "            auroc_val = roc_auc_score(y_val, y_pred_val)\n",
    "            auprc_val = average_precision_score(y_val, y_pred_val)\n",
    "    \n",
    "            # Prediction & Evaluation on holdout set\n",
    "            y_pred_hold = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "            auroc_hold = roc_auc_score(y_test, y_pred_hold)\n",
    "            auprc_hold = average_precision_score(y_test, y_pred_hold)\n",
    "                    \n",
    "            # Collecting metrics\n",
    "            metrics_dict = {'AUROC_val': auroc_val, 'AUPRC_val': auprc_val,\n",
    "                            'AUROC_hold': auroc_hold, 'AUPRC_hold': auprc_hold}\n",
    "            metrics.append(metrics_dict)\n",
    "            print(metrics_dict)\n",
    "            \n",
    "            # Collecting predictions with ids and fold info\n",
    "            predictions.extend(zip(ids_test, y_pred_hold))\n",
    "            \n",
    "            print(outer_fold, inner_fold)\n",
    "            \n",
    "    ##### Save data\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_pickle(f'./ML_Results/Metrics/{dx_name}_{condition}_metrics.pkl')\n",
    "    metrics = 0\n",
    "    metrics_df = 0\n",
    "\n",
    "    imp_df = pd.DataFrame(importances)\n",
    "    imp_df.to_pickle(f'./ML_Results/Importance/{dx_name}_{condition}_gain.pkl')\n",
    "    #imp_df.mean().reset_index().to_excel('fi.xlsx')\n",
    "    importances = 0\n",
    "    imp_df = 0\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['f.eid', 'Prediction']).groupby('f.eid')['Prediction'].mean().reset_index()\n",
    "    predictions_df = predictions_df.merge(indata[['f.eid',dx_name]])\n",
    "    predictions_df.to_pickle(f'./ML_Results/Predictions/{dx_name}_{condition}_hold_predictions.pkl')\n",
    "    predictions = 0\n",
    "    predictions_df = 0\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for shap_array, ids in zip(hold_shap, hold_ids):\n",
    "        df = pd.DataFrame(shap_array, columns=col_names)\n",
    "        df['f.eid'] = np.array(ids).flatten()\n",
    "        dfs.append(df)\n",
    "\n",
    "    dfs = pd.concat(dfs, ignore_index=True)\n",
    "    dfs = dfs.groupby('f.eid').mean().reset_index()\n",
    "    dfs.to_pickle(f'./ML_Results/Importance/{dx_name}_{condition}_shap.pkl')\n",
    "    hold_shap = 0\n",
    "    hold_ids = 0\n",
    "    dfs = 0\n",
    "\n",
    "    train_data = 0\n",
    "    val_data = 0\n",
    "    X_train_val = 0\n",
    "    X_train = 0\n",
    "    X_val = 0\n",
    "    X_test = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML with MR-selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dx in ['afib','cad','celiac','gallstone','polyp','t2d','varicose','vte']:\n",
    "\n",
    "    dx_name = dx+'_status'\n",
    "\n",
    "    cd = pd.read_pickle('./Final/cases.pkl')\n",
    "    cond = pd.read_pickle('./MR_Results/mr_conditions.pkl')\n",
    "    all_feat = ['f.eid','sex','age','Fasting time'] + list(cond.loc[(cond['Disease'] == dx) & (cond['Condition'] == 'All MR tested')]['Features'].iloc[0])\n",
    "    indata = pd.read_pickle('./Final/indata.pkl')[all_feat]\n",
    "\n",
    "    indata = indata.merge(cd[['f.eid',dx_name]], on='f.eid', how='inner').drop_duplicates('f.eid')\n",
    "    indata = indata.loc[indata[dx_name] != -1]\n",
    "    indata.loc[indata[dx_name].isna(), dx_name] = 0\n",
    "\n",
    "    if dx == 'afib':\n",
    "        indata = indata.drop(['I48','C01A','C01B','B01A'],axis=1,errors='ignore')\n",
    "    elif dx == 'cad':\n",
    "        indata = indata.drop(['C01D','I20','I21','I22','I23','I24','I25'],axis=1,errors='ignore')\n",
    "    elif dx == 'celiac':\n",
    "        indata = indata.drop(['K90'],axis=1,errors='ignore')\n",
    "    elif dx == 'gallstone':\n",
    "        indata = indata.drop(['K80'],axis=1,errors='ignore')\n",
    "    elif dx == 'polyp':\n",
    "        indata = indata.drop(['J33'],axis=1,errors='ignore')\n",
    "    elif dx == 't2d':\n",
    "        indata = indata.drop(['E11','E14','A10B'],axis=1,errors='ignore')\n",
    "    elif dx == 'varicose':\n",
    "        indata = indata.drop(['I83'],axis=1,errors='ignore')\n",
    "    elif dx == 'vte':\n",
    "        indata = indata.drop(['I80','I81','I82','I26','B01A'],axis=1,errors='ignore')\n",
    "        \n",
    "    indata = indata.dropna(thresh=100,axis=1)\n",
    "\n",
    "    ##### Run ML\n",
    "    \n",
    "    for condition in cond['Condition'].unique():\n",
    "        print(condition)\n",
    "\n",
    "        feat_inc = ['age', 'sex', 'Fasting time'] + list(cond.loc[(cond['Disease'] == dx) & (cond['Condition'] == condition)]['Features'].iloc[0])\n",
    "        feat_inc = [col for col in feat_inc if col in indata.columns]\n",
    "\n",
    "        X = indata.drop(['f.eid', dx_name],axis=1)[feat_inc].astype(float)\n",
    "        col_names = X.columns.to_list()\n",
    "        X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_.\\ ]+', '', x))\n",
    "        cat_feat = id_cat_feat(X)\n",
    "        y = indata[dx_name].astype(float)\n",
    "        ids = indata['f.eid']\n",
    "\n",
    "        # Placeholder for metrics and predictions\n",
    "        metrics = []\n",
    "        predictions = []\n",
    "        importances = []\n",
    "        hold_shap = []\n",
    "        hold_ids = []\n",
    "\n",
    "        # Outer 6-fold CV\n",
    "        outer_cv = KFold(n_splits=6, shuffle=True)\n",
    "        for outer_fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(X)):\n",
    "            X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "            y_train_val, y_test = y.iloc[train_val_idx], y.iloc[test_idx]\n",
    "            ids_test = ids.iloc[test_idx]\n",
    "\n",
    "            # Inner 6-fold CV (90% train/validation, 10% validation)\n",
    "            inner_cv = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "            for inner_fold, (train_idx, val_idx) in enumerate(inner_cv.split(X_train_val)):\n",
    "                X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "                y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "                \n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "                \n",
    "                params = {\n",
    "                    'data_sample_strategy': 'goss',\n",
    "                    'boosting': 'gbdt',\n",
    "                    'objective': 'binary',\n",
    "                    'force_col_wise': 'true',\n",
    "                    'num_iterations': 1000,\n",
    "                    'num_leaves': 50,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'min_data_in_leaf': 100,\n",
    "                    'early_stopping_round': 5,\n",
    "                    'lambda_l2': 1,\n",
    "                    'verbose': 0,\n",
    "                    'feature_fraction': 1\n",
    "                }\n",
    "            \n",
    "                # Model training\n",
    "                if True:\n",
    "                    model = lgb.train(params, train_data, valid_sets=val_data, categorical_feature=cat_feat)\n",
    "                    model.save_model(f'./ML_Results/Models/{dx_name}_{condition}_{outer_fold}_{inner_fold}.model', num_iteration=model.best_iteration)\n",
    "                \n",
    "                if False:\n",
    "                    model = lgb.Booster(model_file=f'./ML_Results/Models/{dx_name}_{condition}_{outer_fold}_{inner_fold}.model')\n",
    "                \n",
    "                importances.append(dict(zip(col_names, model.feature_importance(importance_type='gain'))))\n",
    "                if inner_fold == 0:\n",
    "                    hold_shap.append(model.predict(X_test, num_iteration=model.best_iteration, pred_contrib=True)[:, :-1])\n",
    "                    hold_ids.append(ids_test)    \n",
    "                \n",
    "                # Prediction & Evaluation on validation set\n",
    "                y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "                auroc_val = roc_auc_score(y_val, y_pred_val)\n",
    "                auprc_val = average_precision_score(y_val, y_pred_val)\n",
    "        \n",
    "                # Prediction & Evaluation on holdout set\n",
    "                y_pred_hold = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "                auroc_hold = roc_auc_score(y_test, y_pred_hold)\n",
    "                auprc_hold = average_precision_score(y_test, y_pred_hold)\n",
    "                        \n",
    "                # Collecting metrics\n",
    "                metrics_dict = {'AUROC_val': auroc_val, 'AUPRC_val': auprc_val,\n",
    "                                'AUROC_hold': auroc_hold, 'AUPRC_hold': auprc_hold}\n",
    "                metrics.append(metrics_dict)\n",
    "                print(metrics_dict)\n",
    "                \n",
    "                # Collecting predictions with ids and fold info\n",
    "                predictions.extend(zip(ids_test, y_pred_hold))\n",
    "                \n",
    "                print(outer_fold, inner_fold)\n",
    "                \n",
    "        ##### Save data\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        metrics_df.to_pickle(f'./ML_Results/Metrics/{dx_name}_{condition}_metrics.pkl')\n",
    "        metrics = 0\n",
    "        metrics_df = 0\n",
    "\n",
    "        imp_df = pd.DataFrame(importances)\n",
    "        imp_df.to_pickle(f'./ML_Results/Importance/{dx_name}_{condition}_gain.pkl')\n",
    "        #imp_df.mean().reset_index().to_excel('fi.xlsx')\n",
    "        importances = 0\n",
    "        imp_df = 0\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions, columns=['f.eid', 'Prediction']).groupby('f.eid')['Prediction'].mean().reset_index()\n",
    "        predictions_df = predictions_df.merge(indata[['f.eid',dx_name]])\n",
    "        predictions_df.to_pickle(f'./ML_Results/Predictions/{dx_name}_{condition}_hold_predictions.pkl')\n",
    "        predictions = 0\n",
    "        predictions_df = 0\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for shap_array, ids in zip(hold_shap, hold_ids):\n",
    "            df = pd.DataFrame(shap_array, columns=col_names)\n",
    "            df['f.eid'] = np.array(ids).flatten()\n",
    "            dfs.append(df)\n",
    "\n",
    "        dfs = pd.concat(dfs, ignore_index=True)\n",
    "        dfs = dfs.groupby('f.eid').mean().reset_index()\n",
    "        dfs.to_pickle(f'./ML_Results/Importance/{dx_name}_{condition}_shap.pkl')\n",
    "        hold_shap = 0\n",
    "        hold_ids = 0\n",
    "        dfs = 0\n",
    "\n",
    "        train_data = 0\n",
    "        val_data = 0\n",
    "        X_train_val = 0\n",
    "        X_train = 0\n",
    "        X_val = 0\n",
    "        X_test = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['All features','All MR tested',\n",
    "              'Any p < 0.05','No p < 0.05',\n",
    "              'Any p < 0.01', 'Any p < 0.001', 'Any Bonferroni',\n",
    "              'IVW p < 0.05', 'IVW p < 0.01', 'IVW p < 0.001', 'IVW Bonferroni',\n",
    "              'IVW p < 0.05 + 3 robust', 'IVW p < 0.01 + 3 robust','IVW p < 0.001 + 3 robust', 'IVW Bonferroni + 3 robust']\n",
    "\n",
    "for dx in ['afib','cad','celiac','gallstone','polyp','t2d','varicose','vte']:\n",
    "    dx_name = dx+'_status'\n",
    "    for cond in conditions:\n",
    "        hp = pd.read_pickle(f'/Users/robchiral/Desktop/MR_ML/ML_Results/Predictions/{dx_name}_{cond}_hold_predictions.pkl')\n",
    "        auroc = bootstrap(hp, 'Prediction', dx_name, roc_auc_score, n_bootstrap=500)\n",
    "        auprc = bootstrap(hp, 'Prediction', dx_name, average_precision_score, n_bootstrap=500)\n",
    "        brier = bootstrap(hp, 'Prediction', dx_name, brier_score_loss, n_bootstrap=500)\n",
    "        logloss = bootstrap(hp, 'Prediction', dx_name, log_loss, n_bootstrap=500)\n",
    "        \n",
    "        bs_metrics = pd.DataFrame([{\n",
    "            'Outcome': dx,\n",
    "            'Condition': cond,\n",
    "            'AUROC': auroc[0], 'AUROC_5': auroc[1], 'AUROC_95': auroc[2],\n",
    "            'AUPRC': auprc[0], 'AUPRC_5': auprc[1], 'AUPRC_95': auprc[2],\n",
    "            'Brier': brier[0], 'Brier_5': brier[1], 'Brier_95': brier[2],\n",
    "            'LogLoss': logloss[0], 'LogLoss_5': logloss[1], 'LogLoss_95': logloss[2]\n",
    "        }])\n",
    "        \n",
    "        bs_metrics.to_csv(f'./ML_Results/Bootstrap/Individual/{dx_name}_{cond}_bs_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
